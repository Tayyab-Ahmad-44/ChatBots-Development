{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Your Damn Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man is to Computer Programmer as Woman is to\n",
      "Homemaker? Debiasing Word Embeddings\n",
      "Tolga Bolukbasi1,Kai-Wei Chang2,James Zou2,Venkatesh Saligrama1,2,Adam Kalai2\n",
      "1Boston University, 8 Saint Mary’s Street, Boston, MA\n",
      "2Microsoft Research New England, 1 Memorial Drive, Cambridge, MA\n",
      "tolgab@bu.edu, kw@kwchang.net, jamesyzou@gmail.com, srv@bu.edu, adam.kalai@microsoft.com\n",
      "Abstract\n",
      "The blind application of machine learning runs the risk of amplifying biases present\n",
      "in data. Such a danger is facing us with word embedding , a popular framework to\n",
      "represent text data as vectors which has been used in many machine learning and\n",
      "natural language processing tasks. We show that even word embeddings trained on\n",
      "Google News articles exhibit female/male gender stereotypes to a disturbing extent.\n",
      "This raises concerns because their widespread use, as we describe, often tends to\n",
      "amplify these biases. Geometrically, gender bias is ﬁrst shown to be captured by\n",
      "a direction in the word embedding. Second, gender neutral words are shown to\n",
      "be linearly separable from gender deﬁnition words in the word embedding. Using\n",
      "these properties, we provide a methodology for modifying an embedding to remove\n",
      "gender stereotypes, such as the association between the words receptionist and\n",
      "female , while maintaining desired associations such as between the words queen\n",
      "andfemale . Using crowd-worker evaluation as well as standard benchmarks, we\n",
      "empirically demonstrate that our algorithms signiﬁcantly reduce gender bias in\n",
      "embeddings while preserving the its useful properties such as the ability to cluster\n",
      "related concepts and to solve analogy tasks. The resulting embeddings can be used\n",
      "in applications without amplifying gender bias.\n",
      "1 Introduction\n",
      "Research on word embeddings has drawn signiﬁcant interest in machine learning and natural language\n",
      "processing. There have been hundreds of papers written about word embeddings and their applications,\n",
      "from Web search [ 22] to parsing Curriculum Vitae [ 12]. However, none of these papers have\n",
      "recognized how blatantly sexist the embeddings are and hence risk introducing biases of various\n",
      "types into real-world systems.\n",
      "A word embedding, trained on word co-occurrence in text corpora, represents each word (or common\n",
      "phrase) was ad-dimensional word vector ~w2Rd. It serves as a dictionary of sorts for computer\n",
      "programs that would like to use word meaning. First, words with similar semantic meanings tend to\n",
      "have vectors that are close together. Second, the vector differences between words in embeddings\n",
      "have been shown to represent relationships between words [ 27,21]. For example given an analogy\n",
      "puzzle, “man is to king as woman is to x” (denoted as man:king ::woman :x), simple arithmetic of\n",
      "the embedding vectors ﬁnds that x=queen is the best answer because\u0000\u0000!man\u0000\u0000\u0000\u0000\u0000!woman ⇡\u0000\u0000!king \u0000\u0000\u0000\u0000!queen .\n",
      "Similarly, x=Japan is returned for Paris :France ::Tokyo :x. It is surprising that a simple vector\n",
      "arithmetic can simultaneously capture a variety of relationships. It has also excited practitioners\n",
      "because such a tool could be useful across applications involving natural language. Indeed, they\n",
      "are being studied and used in a variety of downstream applications (e.g., document ranking [ 22],\n",
      "sentiment analysis [14], and question retrieval [17]).\n",
      "However, the embeddings also pinpoint sexism implicit in text. For instance, it is also the case that:\n",
      "\u0000\u0000!man\u0000\u0000\u0000\u0000\u0000!woman ⇡\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000!computer programmer \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000!homemaker .\n",
      "In other words, the same system that solved the above reasonable analogies will offensively answer\n",
      "“man is to computer programmer as woman is to x” with x=homemaker . Similarly, it outputs that a\n",
      "30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\n",
      "Extremeshe1. homemaker2. nurse3. receptionist4. librarian5. socialite6. hairdresser7. nanny8. bookkeeper9. stylist10. housekeeperExtremehe1. maestro2. skipper3. protege4. philosopher5. captain6. architect7. ﬁnancier8. warrior9. broadcaster10. magicianGender stereotypeshe-heanalogiessewing-carpentry registered nurse-physician housewife-shopkeepernurse-surgeon interior designer-architect softball-baseballblond-burly feminism-conservatism cosmetics-pharmaceuticalsgiggle-chuckle vocalist-guitarist petite-lankysassy-snappy diva-superstar charming-affablevolleyball-football cupcakes-pizzas lovely-brilliantGender appropriateshe-heanalogiesqueen-king sister-brother mother-fatherwaitress-waiter ovarian cancer-prostate cancer convent-monasteryFigure 1: LeftThe most extreme occupations as projected on to the she\u0000hegender direction on\n",
      "w2vNEWS. Occupations such as businesswoman , where gender is suggested by the orthography,\n",
      "were excluded. Right Automatically generated analogies for the pair she-he using the procedure\n",
      "described in text. Each automatically generated analogy is evaluated by 10 crowd-workers to whether\n",
      "or not it reﬂects gender stereotype.\n",
      "father is to a doctor as amother is to a nurse . The primary embedding studied in this paper is the\n",
      "popular publicly-available word2vec [ 19,20] 300 dimensional embedding trained on a corpus of\n",
      "Google News texts consisting of 3 million English words, which we refer to here as the w2vNEWS.\n",
      "One might have hoped that the Google News embedding would exhibit little gender bias because\n",
      "many of its authors are professional journalists. We also analyze other publicly available embeddings\n",
      "trained via other algorithms and ﬁnd similar biases (Appendix B).\n",
      "In this paper, we quantitatively demonstrate that word-embeddings contain biases in their geometry\n",
      "that reﬂect gender stereotypes present in broader society.1Due to their wide-spread usage as basic\n",
      "features, word embeddings not only reﬂect such stereotypes but can also amplify them. This poses a\n",
      "signiﬁcant risk and challenge for machine learning and its applications. The analogies generated from\n",
      "these embeddings spell out the bias implicit in the data on which they were trained. Hence, word\n",
      "embeddings may serve as a means to extract implicit gender associations from a large text corpus\n",
      "similar to how Implicit Association Tests [ 11] detect automatic gender associations possessed by\n",
      "people, which often do not align with self reports.\n",
      "To quantify bias, we will compare a word vector to the vectors of a pair of gender-speciﬁc words. For\n",
      "instance, the fact that\u0000\u0000 \u0000!nurse is close to\u0000\u0000\u0000\u0000!woman is not in itself necessarily biased(it is also somewhat\n",
      "close to\u0000\u0000!man – all are humans), but the fact that these distances are unequal suggests bias. To make\n",
      "this rigorous, consider the distinction between gender speciﬁc words that are associated with a gender\n",
      "by deﬁnition, and the remaining gender neutral words. Standard examples of gender speciﬁc words\n",
      "include brother ,sister ,businessman andbusinesswoman . We will use the gender speciﬁc words to\n",
      "learn a gender subspace in the embedding, and our debiasing algorithm removes the bias only from\n",
      "the gender neutral words while respecting the deﬁnitions of these gender speciﬁc words.\n",
      "We propose approaches to reduce gender biases in the word embedding while preserving the useful\n",
      "properties of the embedding. Surprisingly, not only does the embedding capture bias, but it also\n",
      "contains sufﬁcient information to reduce this bias.We will leverage the fact that there exists a low\n",
      "dimensional subspace in the embedding that empirically captures much of the gender bias.\n",
      "2 Related work and Preliminary\n",
      "Gender bias and stereotype in English. It is important to quantify and understand bias in languages\n",
      "as such biases can reinforce the psychological status of different groups [ 28]. Gender bias in language\n",
      "has been studied over a number of decades in a variety of contexts (see, e.g., [ 13]) and we only\n",
      "has been studied over a number of decades in a variety of contexts (see, e.g., [ 13]) and we only\n",
      "highlight some of the ﬁndings here. Biases differ across people though commonalities can be detected.\n",
      "Implicit Association Tests [ 11] have uncovered gender-word biases that people do not self-report and\n",
      "may not even be aware of. Common biases link female terms with liberal arts and family and male\n",
      "terms with science and careers [ 23]. Bias is seen in word morphology, i.e., the fact that words such as\n",
      "1Stereotypes are biases that are widely held among a group of people. We show that the biases in the word\n",
      "embedding are in fact closely aligned with social conception of gender stereotype, as evaluated by U.S.-based\n",
      "crowd workers on Amazon’s Mechanical Turk. The crowd agreed that the biases reﬂected both in the location of\n",
      "vectors (e.g.\u0000\u0000\u0000!doctor closer to\u0000\u0000!man than to\u0000\u0000\u0000\u0000!woman ) as well as in analogies (e.g., he:coward ::she:whore .) exhibit\n",
      "common gender stereotypes.\n",
      "2\n",
      "actor are, by default, associated with the dominant class [ 15], and female versions of these words,\n",
      "e.g.,actress , are marked. There is also an imbalance in the number of words with F-M with various\n",
      "associations. For instance, while there are more words referring to males, there are many more words\n",
      "that sexualize females than males [ 30]. Consistent biases have been studied within online contexts\n",
      "and speciﬁcally related to the contexts we study such as online news (e.g., [ 26]), Web search (e.g.,\n",
      "[16]), and Wikipedia (e.g., [34]).\n",
      "Bias within algorithms. A number of online systems have been shown to exhibit various biases,\n",
      "such as racial discrimination and gender bias in the ads presented to users [ 31,4]. A recent study\n",
      "found that algorithms used to predict repeat offenders exhibit indirect racial biases [ 1]. Different\n",
      "demographic and geographic groups also use different dialects and word-choices in social media\n",
      "[6]. An implication of this effect is that language used by minority group might not be able to be\n",
      "processed by natural language tools that are trained on “standard” data-sets. Biases in the curation of\n",
      "machine learning data-sets have explored in [32, 3].\n",
      "Independent from our work, Schmidt [ 29] identiﬁed the bias present in word embeddings and\n",
      "proposed debiasing by entirely removing multiple gender dimensions, one for each gender pair. His\n",
      "goal and approach, similar but simpler than ours, was to entirely remove gender from the embedding.\n",
      "There is also an intense research agenda focused on improving the quality of word embeddings from\n",
      "different angles (e.g., [ 18,25,35,7]), and the difﬁculty of evaluating embedding quality (as compared\n",
      "to supervised learning) parallels the difﬁculty of deﬁning bias in an embedding.\n",
      "Within machine learning, a body of notable work has focused on “fair” binary classiﬁcation in\n",
      "particular. A deﬁnition of fairness based on legal traditions is presented by Barocas and Selbst [ 2].\n",
      "Approaches to modify classiﬁcation algorithms to deﬁne and achieve various notions of fairness\n",
      "have been described in a number of works, see, e.g., [ 2,5,8] and a recent survey [ 36]. The prior\n",
      "work on algorithmic fairness is largely for supervised learning. Fair classiﬁcation is deﬁned based\n",
      "on the fact that algorithms were classifying a set of individuals using a set of features with a\n",
      "distinguished sensitive feature. In word embeddings, there are no clear individuals and no a priori\n",
      "deﬁned classiﬁcation problem. However, similar issues arise, such as direct and indirect bias [24].\n",
      "Word embedding. An embedding consists of a unit vector ~w2Rd, with k~wk=1, for each word\n",
      "(or term) w2W. We assume there is a set of gender neutral words N⇢W, such as ﬂight attendant\n",
      "orshoes , which, by deﬁnition, are not speciﬁc to any gender. We denote the size of a set Sby|S|.W e\n",
      "also assume we are given a set of F-M gender pairs P⇢W⇥W, such as she-he ormother-father\n",
      "whose deﬁnitions differ mainly in gender. Section 5 discusses how NandPcan be found within\n",
      "the embedding itself, but until then we take them as given. As is common, similarity between two\n",
      "vectors uandvcan be measured by their cosine similarity :cos(u, v)=u·v\n",
      "kukkvk.This normalized\n",
      "similarity between vectors uandvis the cosine of the angle between the two vectors. Since words\n",
      "are normalized cos(~w1,~ w2)=~w1·~w2.2\n",
      "Unless otherwise stated, the embedding we refer to is the aforementioned w2vNEWS embedding, a\n",
      "d= 300 -dimensional word2vec [ 19,20] embedding, which has proven to be immensely useful since\n",
      "it is high quality, publicly available, and easy to incorporate into any application. In particular, we\n",
      "downloaded the pre-trained embedding on the Google News corpus,3and normalized each word to\n",
      "unit length as is common. Starting with the 50,000 most frequent words, we selected only lower-case\n",
      "words and phrases consisting of fewer than 20 lower-case characters (words with upper-case letters,\n",
      "unit length as is common. Starting with the 50,000 most frequent words, we selected only lower-case\n",
      "words and phrases consisting of fewer than 20 lower-case characters (words with upper-case letters,\n",
      "digits, or punctuation were discarded). After this ﬁltering, 26,377 words remained. While we focus\n",
      "on w2vNEWS, we show later that gender stereotypes are also present in other embedding data-sets.\n",
      "Crowd experiments.4Two types of experiments were performed: ones where we solicited words\n",
      "from the crowd (to see if the embedding biases contain those of the crowd) and ones where we\n",
      "solicited ratings on words or analogies generated from our embedding (to see if the crowd’s biases\n",
      "contain those from the embedding). These two types of experiments are analogous to experiments\n",
      "performed in rating results in information retrieval to evaluate precision and recall. When we speak\n",
      "of the majority of 10 crowd judgments, we mean those annotations made by 5 or more independent\n",
      "workers. The Appendix contains the questionnaires that were given to the crowd-workers.\n",
      "2We will abuse terminology and refer to the embedding of a word and the word interchangeably. For example,\n",
      "the statement catis more similar to dogthan to cowmeans\u0000!cat·\u0000!dog\u0000\u0000!cat·\u0000\u0000!cow.\n",
      "3https://code.google.com/archive/p/word2vec/\n",
      "4All human experiments were performed on the Amazon Mechanical Turk platform. We selected for\n",
      "U.S.-based workers to maintain homogeneity and reproducibility to the extent possible with crowdsourcing.\n",
      "3\n",
      "3 Geometry of Gender and Bias in Word Embeddings\n",
      "Our ﬁrst task is to understand the biases present in the word-embedding (i.e. which words are closer\n",
      "toshethan to he, etc.) and the extent to which these geometric biases agree with human notion of\n",
      "gender stereotypes. We use two simple methods to approach this problem: 1) evaluate whether the\n",
      "embedding has stereotypes on occupation words and 2) evaluate whether the embedding produces\n",
      "analogies that are judged to reﬂect stereotypes by humans. The exploratory analysis of this section\n",
      "will motivate the more rigorous metrics used in the next two sections.\n",
      "Occupational stereotypes. Figure 1 lists the occupations that are closest to sheand to hein the\n",
      "w2vNEWS embeddings. We asked the crowdworkers to evaluate whether an occupation is considered\n",
      "female-stereotypic, male-stereotypic, or neutral. The projection of the occupation words onto the she-\n",
      "heaxis is strongly correlated with the stereotypicality estimates of these words (Spearman ⇢=0.51),\n",
      "suggesting that the geometric biases of embedding vectors is aligned with crowd judgment. We\n",
      "projected each of the occupations onto the she-he direction in the w2vNEWS embedding as well as a\n",
      "different embedding generated by the GloVe algorithm on a web-crawl corpus [ 25]. The results are\n",
      "highly consistent (Appendix Figure 6), suggesting that gender stereotypes is prevalent across different\n",
      "embeddings and is not an artifact of the particular training corpus or methodology of word2vec.\n",
      "Analogies exhibiting stereotypes. Analogies are a useful way to both evaluate the quality of a word\n",
      "embedding and also its stereotypes. We ﬁrst brieﬂy describe how the embedding generate analogies\n",
      "and then discuss how we use analogies to quantify gender stereotype in the embedding. A more\n",
      "detailed discussion of our algorithm and prior analogy solvers is given in Appendix C.\n",
      "In the standard analogy tasks, we are given three words, for example he, she, king , and look for the\n",
      "4th word to solve hetoking is as shetox. Here we modify the analogy task so that given two words,\n",
      "e.g.he, she , we want to generate a pair of words, xandy, such that hetoxasshetoyis a good\n",
      "analogy. This modiﬁcation allows us to systematically generate pairs of words that the embedding\n",
      "believes it analogous to he, she (or any other pair of seed words). The input into our analogy generator\n",
      "is a seed pair of words (a, b)determining a seed direction ~a\u0000~bcorresponding to the normalized\n",
      "difference between the two seed words. In the task below, we use (a, b)=( she,he). We then score\n",
      "all pairs of words x, yby the following metric:\n",
      "S(a,b)(x, y) = cos⇣\n",
      "~a\u0000~b, ~ x\u0000~y⌘\n",
      "ifk~x\u0000~yk\u0000,0else (1)\n",
      "where \u0000is a threshold for similarity. The intuition of the scoring metric is that we want a good\n",
      "analogy pair to be close to parallel to the seed direction while the two words are not too far apart in\n",
      "order to be semantically coherent. The parameter \u0000sets the threshold for semantic similarity. In all\n",
      "the experiments, we take \u0000=1as we ﬁnd that this choice often works well in practice. Since all\n",
      "embeddings are normalized, this threshold corresponds to an angle ⇡/3, indicating that the two\n",
      "words are closer to each other than they are to the origin. In practice, it means that the two words\n",
      "forming the analogy are signiﬁcantly closer together than two random embedding vectors. Given the\n",
      "embedding and seed words, we output the top analogous pairs with the largest positive S(a,b)scores.\n",
      "To reduce redundancy, we do not output multiple analogies sharing the same word x.\n",
      "We employed U.S. based crowd-workers to evaluate the analogies output by the aforementioned\n",
      "algorithm. For each analogy, we asked the workers two yes/no questions: (a) whether the pairing\n",
      "makes sense as an analogy, and (b) whether it reﬂects a gender stereotype. Overall, 72 out of 150\n",
      "analogies were rated as gender-appropriate by ﬁve or more out of 10 crowd-workers, and 29 analogies\n",
      "makes sense as an analogy, and (b) whether it reﬂects a gender stereotype. Overall, 72 out of 150\n",
      "analogies were rated as gender-appropriate by ﬁve or more out of 10 crowd-workers, and 29 analogies\n",
      "were rated as exhibiting gender stereotype by ﬁve or more crowd-workers (Figure 4). Examples of\n",
      "analogies generated from w2vNEWS are shown at Figure 1. The full list are in Appendix J.\n",
      "Identifying the gender subspace. Next, we study the bias present in the embedding geometrically,\n",
      "identifying the gender direction and quantifying the bias independent of the extent to which it is\n",
      "aligned with the crowd bias. Language use is “messy” and therefore individual word pairs do not\n",
      "always behave as expected. For instance, the word man has several different usages: it may be used\n",
      "as an exclamation as in oh man! or to refer to people of either gender or as a verb, e.g., man the\n",
      "station . To more robustly estimate bias, we shall aggregate across multiple paired comparisons. By\n",
      "combining several directions, such as\u0000!she\u0000\u0000!heand\u0000\u0000\u0000\u0000!woman \u0000\u0000\u0000!man, we identify a gender direction\n",
      "g2Rdthat largely captures gender in the embedding. This direction helps us to quantify direct and\n",
      "indirect biases in words and associations.\n",
      "In English as in many languages, there are numerous gender pair terms, and for each we can\n",
      "consider the difference between their embeddings. Before looking at the data, one might imagine\n",
      "4\n",
      "def. stereo.\u0000!she\u0000\u0000!he 92% 89%\u0000!her\u0000\u0000!his 84% 87%\u0000\u0000\u0000\u0000!woman\u0000\u0000\u0000!man 90% 83%\u0000\u0000 \u0000!Mary\u0000\u0000\u0000!John 75% 87%\u0000\u0000 \u0000 \u0000!herself\u0000\u0000\u0000\u0000\u0000!himself 93% 89%\u0000\u0000\u0000\u0000\u0000!daughter\u0000\u0000!son 93% 91%\u0000\u0000 \u0000 \u0000!mother\u0000\u0000\u0000\u0000!father 91% 85%\u0000!gal\u0000\u0000!guy 85% 85%\u0000!girl\u0000\u0000!boy 90% 86%\u0000\u0000 \u0000 \u0000!female\u0000\u0000\u0000!male 84% 75%\n",
      "RG WS analogyBefore62.3 54.5 57.0Hard-debiased62.4 54.1 57.0Soft-debiased62.4 54.2 56.8Figure 2:Left:Ten word pairs to deﬁne gender, along with agreement with sets of deﬁnitionaland stereotypical words solicited from the crowd. The accuracy is shown for the correspondinggender classiﬁer based on which word is closer to a target word, e.g., theshe-heclassiﬁer predicts aword is female if it is closer toshethanhe.Middle:The bar plot shows the percentage of varianceexplained in the PCA of the 10 pairs of gender words. The top component explains signiﬁcantly morevariance than any other; the corresponding percentages for random words shows a more gradual decay(Figure created by averaging over 1,000 draws of ten random unit vectors in 300 dimensions).Right:The table shows performance of the original w2vNEWS embedding (“before”) and the debiasedw2vNEWS on standard evaluation metrics measuring coherence and analogy-solving abilities: RG[27], WS [10], MSR-analogy [21]. Higher is better. The results show that the performance does notdegrade after debiasing. Note that we use a subset of vocabulary in the experiments. Therefore, theperformances are lower than the previously published results. See Appendix for full results.that they all had roughly the same vector differences, as in the following caricature:\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000!grandmother=\u0000\u0000!wise+\u0000!gal,\u0000\u0000\u0000\u0000\u0000\u0000\u0000!grandfather=\u0000\u0000!wise+\u0000!guy,\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000!grandmother\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000!grandfather=\u0000!gal\u0000\u0000!guy=gHowever, genderpair differences are not parallel in practice, for multiple reasons. First, there are different biasesassociated with with different gender pairs. Second is polysemy, as mentioned, which in this caseoccurs due to the other use ofgrandfatheras into grandfather a regulation. Finally, randomness inthe word counts in any ﬁnite sample will also lead to differences. Figure 2 illustrates ten possiblegender pairs,\u0000(xi,yi) 10i=1.To identify the gender subspace, we took the ten gender pair difference vectors and computed itsprincipal components (PCs). As Figure 2 shows, there is a single direction that explains the majorityof variance in these vectors. The ﬁrst eigenvalue is signiﬁcantly larger than the rest. Note that,from the randomness in a ﬁnite sample of ten noisy vectors, one expects a decrease in eigenvalues.However, as also illustrated in 2, the decrease one observes due to random sampling is much moregradual and uniform. Therefore we hypothesize that the top PC, denoted by the unit vectorg, capturesthe gender subspace. In general, the gender subspace could be higher dimensional and all of ouranalysis and algorithms (described below) work with general subspaces.Direct bias.To measure direct bias, we ﬁrst identify words that should be gender-neutral for theapplication in question. How to generate this set of gender-neutral words is described in Section 5.Given the gender neutral words, denoted byN, and the gender direction learned from above,g, wedeﬁne the direct gender bias of an embedding to be1|N|Pw2N|cos(~w , g)|c, wherecis a parameterthat determines howstrictdo we want to in measuring bias. Ifcis 0, then|cos(~w\u0000g)|c=0only if~whas no overlap withgand otherwise it is 1. Such strict measurement of bias might bedesirable in settings such as the college admissions example from the Introduction, where it wouldbe unacceptable for the embedding to introduce a slight preference for one candidate over anotherby gender. A more gradual bias would be settingc=1. The presentation we have chosen favorssimplicity – it would be natural to extend our deﬁnitions to weight words by frequency. For example,in w2vNEWS, if we takeNto be the set of 327 occupations, thenDirectBias1=0.08, whichconﬁrms that many occupation words have substantial component along the gender direction.4 Debiasing algorithmsThe debiasing algorithms are deﬁned in terms of sets of words rather than just pairs, for generality, sothat we can consider other biases such as racial or religious biases. We also assume that we have a setof words to\n",
      "are deﬁned in terms of sets of words rather than just pairs, for generality, sothat we can consider other biases such as racial or religious biases. We also assume that we have a setof words to neutralize, which can come from a list or from the embedding as described in Section 5.(In many cases it may be easier to list the gender speciﬁc words not to neutralize as this set can bemuch smaller.)5\n",
      "biased\n",
      "okayheFigure 3: Selected words projected along two axes: xis a projection onto the difference between\n",
      "the embeddings of the words heandshe, and yis a direction learned in the embedding that captures\n",
      "gender neutrality, with gender neutral words above the line and gender speciﬁc words below the line.\n",
      "Our hard debiasing algorithm removes the gender pair associations for gender neutral words. In this\n",
      "ﬁgure, the words above the horizontal line would all be collapsed to the vertical line.\n",
      "The ﬁrst step, called Identify gender subspace , is to identify a direction (or, more generally, a\n",
      "subspace) of the embedding that captures the bias. For the second step, we deﬁne two options:\n",
      "Neutralize and Equalize orSoften .Neutralize ensures that gender neutral words are zero in the\n",
      "gender subspace. Equalize perfectly equalizes sets of words outside the subspace and thereby\n",
      "enforces the property that any neutral word is equidistant to all words in each equality set. For\n",
      "instance, if {grandmother ,grandfather }and{guy,gal}were two equality sets, then after equalization\n",
      "babysit would be equidistant to grandmother andgrandfather and also equidistant to galandguy,\n",
      "but presumably closer to the grandparents and further from the galandguy. This is suitable for\n",
      "applications where one does not want any such pair to display any bias with respect to neutral words.\n",
      "The disadvantage of Equalize is that it removes certain distinctions that are valuable in certain\n",
      "applications. For instance, one may wish a language model to assign a higher probability to the phrase\n",
      "to grandfather a regulation ) than to grandmother a regulation since grandfather has a meaning that\n",
      "grandmother does not – equalizing the two removes this distinction. The Soften algorithm reduces\n",
      "the differences between these sets while maintaining as much similarity to the original embedding as\n",
      "possible, with a parameter that controls this trade-off.\n",
      "To deﬁne the algorithms, it will be convenient to introduce some further notation. A subspace Bis\n",
      "deﬁned by korthogonal unit vectors B={b1,...,b k}⇢Rd. In the case k=1, the subspace is\n",
      "simply a direction. We denote the projection of a vector vonto Bby,vB=Pk\n",
      "j=1(v·bj)bj.This\n",
      "also means that v\u0000vBis the projection onto the orthogonal subspace.\n",
      "Step 1: Identify gender subspace . Inputs: word sets W, deﬁning sets D1,D2,...,D n⇢W\n",
      "as well as embedding\u0000\n",
      "~w2Rd \n",
      "w2Wand integer parameter k\u00001. Let µi:=P\n",
      "w2Di~w /|Di|\n",
      "be the means of the deﬁning sets. Let the bias subspace Bbe the ﬁrst krows of SVD( C)where\n",
      "C:=Pn\n",
      "i=1P\n",
      "w2Di(~w\u0000µi)T(~w\u0000µi)\u0000\n",
      "|Di|.\n",
      "Step 2a: Hard de-biasing (neutralize and equalize) . Additional inputs: words to neutralize\n",
      "N✓W, family of equality sets E={E1,E2,...,E m}where each Ei✓W. For each word\n",
      "w2N, let~wbe re-embedded to ~w:= (~w\u0000~wB)\u0000\n",
      "k~w\u0000~wBk.For each set E2E, let\n",
      "µ:=P\n",
      "w2Ew/|E|and⌫:=µ\u0000µB. For each w2E, ~ w :=⌫+p\n",
      "1\u0000k⌫k2~wB\u0000µB\n",
      "k~wB\u0000µBk. Finally,\n",
      "output the subspace Band the new embedding\u0000\n",
      "~w2Rd \n",
      "w2W.\n",
      "Equalize equates each set of words outside of Bto their simple average ⌫and then adjusts vectors\n",
      "so that they are unit length. It is perhaps easiest to understand by thinking separately of the two\n",
      "components ~wBand~w?B=~w\u0000~wB. The latter ~w?Bare all simply equated to their average. Within\n",
      "B, they are centered (moved to mean 0) and then scaled so that each ~wis unit length. To motivate\n",
      "why we center, beyond the fact that it is common in machine learning, consider the bias direction\n",
      "being the gender direction ( k=1) and a gender pair such as E={male,female }. As discussed, it\n",
      "6\n",
      "Figure 4: Number of stereotypical (Left) and appropriate (Right) analogies generated by word\n",
      "embeddings before and after debiasing.\n",
      "so happens that both words are positive (female) in the gender direction, though female has a greater\n",
      "projection. One can only speculate as to why this is the case, e.g., perhaps the frequency of text\n",
      "such as male nurse ormale escort orshe was assaulted by the male . However, because female has a\n",
      "greater gender component, after centering the two will be symmetrically balanced across the origin.\n",
      "If instead, we simply scaled each vector’s component in the bias direciton without centering, male\n",
      "andfemale would have exactly the same embedding and we would lose analogies such as father:male\n",
      ":: mother:female . We note that Neutralizing and Equalizing completely remove pair bias.\n",
      "Observation 1. After Steps 1 and 2a, for any gender neutral word wany equality set E, and any two\n",
      "words e1,e22E,~w·~e1=w·~e2andk~w\u0000~e1k=k~w\u0000~e2k. Furthermore, if E=\u0000\n",
      "{x, y}|(x, y)2P \n",
      "are the sets of pairs deﬁning PairBias, then PairBias = 0 .\n",
      "Step 2b: Soft bias correction . Overloading the notation, we let W2Rd⇥|vocab |denote the matrix\n",
      "of all embedding vectors and Ndenote the matrix of the embedding vectors corresponding to gender\n",
      "neutral words. WandNare learned from some corpus and are inputs to the algorithm. The\n",
      "desired debiasing transformation T2Rd⇥dis a linear transformation that seeks to preserve pairwise\n",
      "inner products between all the word vectors while minimizing the projection of the gender neutral\n",
      "words onto the gender subspace. This can be formalized as min Tk(TW)T(TW)\u0000WTWk2\n",
      "F+\n",
      "\u0000k(TN)T(TB)k2\n",
      "F,where Bis the gender subspace learned in Step 1 and \u0000is a tuning parameter\n",
      "that balances the objective of preserving the original embedding inner products with the goal of\n",
      "reducing gender bias. For \u0000large, Twould remove the projection onto Bfrom all the vectors in N,\n",
      "which corresponds exactly to Step 2a. In the experiment, we use \u0000=0.2. The optimization problem\n",
      "is a semi-deﬁnite program and can be solved efﬁciently. The output embedding is normalized to have\n",
      "unit length, ˆW={Tw/kTwk2,w2W}.\n",
      "5 Determining gender neutral words\n",
      "For practical purposes, since there are many fewer gender speciﬁc words, it is more efﬁcient to\n",
      "enumerate the set of gender speciﬁc words Sand take the gender neutral words to be the compliment,\n",
      "N=W\\S. Using dictionary deﬁnitions, we derive a subset S0of 218 words out of the words in\n",
      "w2vNEWS. Recall that this embedding is a subset of 26,377 words out of the full 3 million words\n",
      "in the embedding, as described in Section 2. This base list S0is given in Appendix F. Note that the\n",
      "choice of words is subjective and ideally should be customized to the application at hand.\n",
      "We generalize this list to the entire 3 million words in the Google News embedding using a linear\n",
      "classiﬁer, resulting in the set Sof 6,449 gender-speciﬁc words. More speciﬁcally, we trained a linear\n",
      "Support Vector Machine (SVM) with regularization parameter of C=1.0. We then ran this classiﬁer\n",
      "on the remaining words, taking S=S0[S1, where S1are the words labeled as gender speciﬁc by\n",
      "our classiﬁer among the words in the entire embedding that are not in the 26,377 words of w2vNEWS.\n",
      "Using 10-fold cross-validation to evaluate the accuracy, we ﬁnd an F-score of .627±.102.\n",
      "Figure 3 illustrates the results of the classiﬁer for separating gender-speciﬁc words from gender-\n",
      "neutral words. To make the ﬁgure legible, we show a subset of the words. The x-axis correspond to\n",
      "projection of words onto the\u0000!she\u0000\u0000!hedirection and the y-axis corresponds to the distance from the\n",
      "decision boundary of the trained SVM.\n",
      "7\n",
      "6 Debiasing results\n",
      "We evaluated our debiasing algorithms to ensure that they preserve the desirable properties of the\n",
      "original embedding while reducing both direct and indirect gender biases. First we used the same\n",
      "analogy generation task as before: for both the hard-debiased and the soft-debiased embeddings,\n",
      "we automatically generated pairs of words that are analogous to she-he and asked crowd-workers\n",
      "to evaluate whether these pairs reﬂect gender stereotypes. Figure 4 shows the results. On the initial\n",
      "w2vNEWS embedding, 19% of the top 150 analogies were judged as showing gender stereotypes\n",
      "by a majority of the ten workers. After applying our hard debiasing algorithm, only 6% of the new\n",
      "embedding were judged as stereotypical.\n",
      "As an example, consider the analogy puzzle, hetodoctor is as shetoX. The original embedding\n",
      "returns X=nurse while the hard-debiased embedding ﬁnds X=physician . Moreover the hard-\n",
      "debiasing algorithm preserved gender appropriate analogies such as shetoovarian cancer is as he\n",
      "toprostate cancer . This demonstrates that the hard-debiasing has effectively reduced the gender\n",
      "stereotypes in the word embedding. Figure 4 also shows that the number of appropriate analogies\n",
      "remains similar as in the original embedding after executing hard-debiasing. This demonstrates that\n",
      "that the quality of the embeddings is preserved. The details results are in Appendix J. Soft-debiasing\n",
      "was less effective in removing gender bias. To further conﬁrms the quality of embeddings after\n",
      "debiasing, we tested the debiased embedding on several standard benchmarks that measure whether\n",
      "related words have similar embeddings as well as how well the embedding performs in analogy tasks.\n",
      "Appendix Table 2 shows the results on the original and the new embeddings and the transformation\n",
      "does not negatively impact the performance. In Appendix A, we show how our algorithm also reduces\n",
      "indirect gender bias.\n",
      "7 Discussion\n",
      "Word embeddings help us further our understanding of bias in language. We ﬁnd a single direction\n",
      "that largely captures gender, that helps us capture associations between gender neutral words and\n",
      "gender as well as indirect inequality. The projection of gender neutral words on this direction enables\n",
      "us to quantify their degree of female- or male-bias.\n",
      "To reduce the bias in an embedding, we change the embeddings of gender neutral words, by removing\n",
      "their gender associations. For instance, nurse is moved to to be equally male and female in the\n",
      "direction g. In addition, we ﬁnd that gender-speciﬁc words have additional biases beyond g. For\n",
      "instance, grandmother andgrandfather are both closer to wisdom thangalandguyare, which does not\n",
      "reﬂect a gender difference. On the other hand, the fact that babysit is so much closer to grandmother\n",
      "than grandfather (more than for other gender pairs) is a gender bias speciﬁc to grandmother . By\n",
      "equating grandmother andgrandfather outside of gender, and since we’ve removed gfrom babysit ,\n",
      "both grandmother andgrandfather and equally close to babysit after debiasing. By retaining the\n",
      "gender component for gender-speciﬁc words, we maintain analogies such as she:grandmother\n",
      ":: he:grandfather . Through empirical evaluations, we show that our hard-debiasing algorithm\n",
      "signiﬁcantly reduces both direct and indirect gender bias while preserving the utility of the embedding.\n",
      "We have also developed a soft-embedding algorithm which balances reducing bias with preserving\n",
      "the original distances, and could be appropriate in speciﬁc settings.\n",
      "One perspective on bias in word embeddings is that it merely reﬂects bias in society, and therefore\n",
      "one should attempt to debias society rather than word embeddings. However, by reducing the bias in\n",
      "today’s computer systems (or at least not amplifying the bias), which is increasingly reliant on word\n",
      "embeddings, in a small way debiased word embeddings can hopefully contribute to reducing gender\n",
      "today’s computer systems (or at least not amplifying the bias), which is increasingly reliant on word\n",
      "embeddings, in a small way debiased word embeddings can hopefully contribute to reducing gender\n",
      "bias in society. At the very least, machine learning should not be used to inadvertently amplify these\n",
      "biases, as we have seen can naturally happen.\n",
      "In speciﬁc applications, one might argue that gender biases in the embedding (e.g. computer\n",
      "programmer is closer to he) could capture useful statistics and that, in these special cases, the original\n",
      "biased embeddings could be used. However given the potential risk of having machine learning\n",
      "algorithms that amplify gender stereotypes and discriminations, we recommend that we should err on\n",
      "the side of neutrality and use the debiased embeddings provided here as much as possible.\n",
      "Acknowledgments. The authors thank Tarleton Gillespie and Nancy Baym for numerous helpful\n",
      "discussions.5\n",
      "5This material is based upon work supported in part by NSF Grants CNS-1330008, CCF-1527618, by ONR\n",
      "Grant 50202168, NGA Grant HM1582-09-1-0037 and DHS 2013-ST-061-ED0001\n",
      "8\n",
      "References\n",
      "[1]J. Angwin, J. Larson, S. Mattu, and L. Kirchner. Machine bias: There’s software used across the country\n",
      "to predict future criminals. and it’s biased against blacks., 2016.\n",
      "[2]S. Barocas and A. D. Selbst. Big data’s disparate impact. Available at SSRN 2477899 , 2014.\n",
      "[3]E. Beigman and B. B. Klebanov. Learning with annotation noise. In ACL, 2009.\n",
      "[4]A. Datta, M. C. Tschantz, and A. Datta. Automated experiments on ad privacy settings. Proceedings on\n",
      "Privacy Enhancing Technologies , 2015.\n",
      "[5]C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In Innovations in\n",
      "Theoretical Computer Science Conference , 2012.\n",
      "[6]J. Eisenstein, B. O’Connor, N. A. Smith, and E. P. Xing. Diffusion of lexical change in social media. PLoS\n",
      "ONE , pages 1–13, 2014.\n",
      "[7]M. Faruqui, J. Dodge, S. K. Jauhar, C. Dyer, E. Hovy, and N. A. Smith. Retroﬁtting word vectors to\n",
      "semantic lexicons. In NAACL , 2015.\n",
      "[8]M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian. Certifying and\n",
      "removing disparate impact. In KDD , 2015.\n",
      "[9]C. Fellbaum, editor. Wordnet: An Electronic Lexical Database . The MIT Press, Cambridge, MA, 1998.\n",
      "[10] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. Placing search\n",
      "in context: The concept revisited. In WWW . ACM, 2001.\n",
      "[11] A. G. Greenwald, D. E. McGhee, and J. L. Schwartz. Measuring individual differences in implicit cognition:\n",
      "the implicit association test. Journal of personality and social psychology , 74(6):1464, 1998.\n",
      "[12] C. Hansen, M. Tosik, G. Goossen, C. Li, L. Bayeva, F. Berbain, and M. Rotaru. How to get the best word\n",
      "vectors for resume parsing. In SNN Adaptive Intelligence / Symposium: Machine Learning 2015, Nijmegen.\n",
      "[13] J. Holmes and M. Meyerhoff. The handbook of language and gender , volume 25. John Wiley & Sons,\n",
      "2008.\n",
      "[14] O.˙Irsoy and C. Cardie. Deep recursive neural networks for compositionality in language. In NIPS . 2014.\n",
      "[15] R. Jakobson, L. R. Waugh, and M. Monville-Burston. On language . Harvard Univ Pr, 1990.\n",
      "[16] M. Kay, C. Matuszek, and S. A. Munson. Unequal representation and gender stereotypes in image search\n",
      "results for occupations. In Human Factors in Computing Systems . ACM, 2015.\n",
      "[17] T. Lei, H. Joshi, R. Barzilay, T. Jaakkola, A. M. Katerina Tymoshenko, and L. Marquez. Semi-supervised\n",
      "question retrieval with gated convolutions. In NAACL . 2016.\n",
      "[18] O. Levy and Y. Goldberg. Linguistic regularities in sparse and explicit word representations. In CoNLL ,\n",
      "2014.\n",
      "[19] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient estimation of word representations in vector space.\n",
      "InICLR , 2013.\n",
      "[20] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and\n",
      "phrases and their compositionality. In NIPS .\n",
      "[21] T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic regularities in continuous space word representations. In\n",
      "HLT-NAACL , pages 746–751, 2013.\n",
      "[22] E. Nalisnick, B. Mitra, N. Craswell, and R. Caruana. Improving document ranking with dual word\n",
      "embeddings. In www , April 2016.\n",
      "[23] B. A. Nosek, M. Banaji, and A. G. Greenwald. Harvesting implicit group attitudes and beliefs from a\n",
      "demonstration web site. Group Dynamics: Theory, Research, and Practice , 6(1):101, 2002.\n",
      "[24] D. Pedreshi, S. Ruggieri, and F. Turini. Discrimination-aware data mining. In SIGKDD , pages 560–568.\n",
      "ACM, 2008.\n",
      "[25] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In EMNLP ,\n",
      "2014.\n",
      "[26] K. Ross and C. Carter. Women and news: A long and winding road. Media, Culture & Society , 33(8):1148–\n",
      "1165, 2011.\n",
      "[27] H. Rubenstein and J. B. Goodenough. Contextual correlates of synonymy. Communications of the ACM ,\n",
      "8(10):627–633, 1965.\n",
      "[28] E. Sapir. Selected writings of Edward Sapir in language, culture and personality , volume 342. Univ of\n",
      "California Press, 1985.\n",
      "[29] B. Schmidt. Rejecting the gender binary: a vector-space operation. http://bookworm.benschmidt.\n",
      "California Press, 1985.\n",
      "[29] B. Schmidt. Rejecting the gender binary: a vector-space operation. http://bookworm.benschmidt.\n",
      "org/posts/2015-10-30-rejecting-the-gender-binary.html , 2015.\n",
      "[30] J. P. Stanley. Paradigmatic woman: The prostitute. Papers in language variation , pages 303–321, 1977.\n",
      "[31] L. Sweeney. Discrimination in online ad delivery. Queue , 11(3):10, 2013.\n",
      "[32] A. Torralba and A. Efros. Unbiased look at dataset bias. In CVPR , 2012.\n",
      "[33] P. D. Turney. Domain and function: A dual-space model of semantic relations and compositions. Journal\n",
      "of Artiﬁcial Intelligence Research , pages 533–585, 2012.\n",
      "[34] C. Wagner, D. Garcia, M. Jadidi, and M. Strohmaier. It’s a man’s wikipedia? assessing gender inequality\n",
      "in an online encyclopedia. In Ninth International AAAI Conference on Web and Social Media , 2015.\n",
      "[35] D. Yogatama, M. Faruqui, C. Dyer, and N. A. Smith. Learning word representations with hierarchical\n",
      "sparse coding. In ICML , 2015.\n",
      "[36] I. Zliobaite. A survey on measuring indirect discrimination in machine learning. arXiv preprint\n",
      "arXiv:1511.00148 , 2015.\n",
      "9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "\n",
    "filePath = \"files/Embedding_Paper.pdf\"\n",
    "file_name = os.path.basename(filePath)\n",
    "\n",
    "pdf = PyPDFLoader(filePath)\n",
    "pages = pdf.load_and_split()\n",
    "\n",
    "pdf_text = \"\"\n",
    "\n",
    "# Iterate through pages and extract text\n",
    "for page in pages:\n",
    "    # Extract text with optional sorting for reading order\n",
    "    extracted_page_text = page.page_content\n",
    "    pdf_text += extracted_page_text + \"\\n\"\n",
    "\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split your Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1024, chunk_overlap = 200)\n",
    "\n",
    "chunks = text_splitter.split_text(pdf_text)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings and Save it in FAISS Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 52 embedded chunks to 'faiss-index'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OllamaEmbeddings(model = \"nomic-embed-text\")\n",
    "\n",
    "db = FAISS.from_texts(chunks, embeddings)\n",
    "\n",
    "db.save_local(\"faiss-index\")\n",
    "\n",
    "print(f\"Saved {len(chunks)} embedded chunks to 'faiss-index'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching Query in db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are Debiasing algorithms\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241m.\u001b[39msimilarity_search(query)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# retriever = db.as_retriever()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# retriever.invoke(query)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"What are Debiasing algorithms\"\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go to the main file "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
